=== Changes for new experiments compared to old ===
NOTE: (NUMBER, NUMBER, NUMBER) == (START, STOP, STEP)


- 6.1-NB
	- UseOutlierRemoval: true --> false

- 6.1-NN
	- UseStatisticalFeatureSelector: true --> false
	- UseOutlierRemoval: true --> false
	- ImputationMethod: NONE --> MODE
	- Normalisation: MIN_MAX --> NONE
	- training_method: RFECV --> FIT
	
- 6.2-DT
	- NormalisationMethod: MIN_MAX --> NONE

- 6.2-NB
	- UseTransformer: false --> true
	- OutlierRemovalMethod: AVF --> ODIN
	
	Transformer:
	- UseOneHotEncoding: false
	- ImputationMethod: MODE
	- NormalisationMethod: NONE
	
	
	- training_method: FIT --> CROSS_VALIDATION
	
- 6.2-NN
	- UseStatisticalFeatureSelector: true --> false
	- training_method: RFECV --> CROSS_VALIDATION
	(MIN_MAX is enabled in both, but should not have been in new experiment)
	
- 6.2-RF
	- UseStatisticalFeatureSelector: false --> true
	- ImputationMethod: NONE --> MODE
	- NormalisationMethod: MIN_MAX --> NONE
	
	StatisticalFeatureSelector:
	- score_function: CHI2
	- (GenericUnivariateSelectArgs) param: 75
	
- 6.3-NB
	- UseTransformer: false --> true
	- OutlierRemovalMethod: AVF --> ODIN
	
	Transformer:
	- UseOneHotEncoding: false
	- ImputationMethod: MODE
	- NormalisationMethod: NONE

- 6.3-NN
	- ImputationMethod: NONE --> MODE
	- NormalisationMethod: MIN_MAX --> NONE
	- score_function: CHI2 --> ANOVA_F
	- (GenericUnivariateSelectArgs) param: 50 --> 75
	- training_method: RFECV --> CROSS_VALIDATION

- 7.1-DT
	Candidates:
	- 49500

	Time:
	3 m

	Test performance:
	- threshold			: 0.907
	- distance			: 0.947
	- exact_accuracy 	: 0.546
	- balanced_accuracy	: 0.372

	Config:
	- min_weight_fraction_leaf: (0.0, 0.5, 0.1) --> 0.0
	- min_impurity_decrease: (0.0, 0.2, 0.03) --> 0.0
	- ccp_alpha: (0.0, 0.5, 0.1) --> 0.0

- 7.1-NN
	Candidates:
	672

	Improvements:
	- Fixed L2 regularization
	- Adaptive learning rate
	- Early stopping
	- Fixed NN-tuple generation
	
	Time:
	9.3 h

	Test performance:
	- threshold			: 0.914 
	- distance			: 0.955
	- exact_accuracy 	: 0.579
	- balanced_accuracy	: 0.483

	Config:
	- n_jobs: 10
	- max_iter: 10000
	- layers: (2,8,1)
	- layer_size: [10,15,20,25]
	- alpha: [0.0001, 0.001, 0.01, 0.1]
	- tol: [0.0001]
	- learning_rate_init: [0.1]

- 7.1-RF
	Candidates:
	46080
	
	Time:
	7.1 h
	
	Test performance:
	- threshold			: 0.911 
	- distance			: 0.954
	- exact_accuracy 	: 0.553
	- balanced_accuracy	: 0.423
	
	
- 7.2-DT
	Candidates:
	- n_iter: 433125 --> 4950

	Time:
	15 s
	
	Test performance:
	- threshold			: 0.828
	- distance			: 0.906
	- exact_accuracy 	: 0.328
	- balanced_accuracy	: 0.168

- 7.2-NN
	Candidates:
	- n_iter: 120 --> 67

	Improvements:
	- Fixed L2 regularization
	- Adaptive learning rate
	- Early stopping
	- Fixed NN-tuple generation
	
	Time:
	1.1 h
	
	Test performance:
	- threshold			: 0.921 
	- distance			: 0.958
	- exact_accuracy 	: 0.563
	- balanced_accuracy	: 0.474
	
	Config:
	- n_jobs: 10
	- max_iter: 10000
	- layers: (2,8,1)
	- layer_size: [10,15,20,25]
	- alpha: [0.0001, 0.001, 0.01, 0.1]
	- tol: [0.0001]
	- learning_rate_init: [0.1]

- 7.2-RF
	Candidates:
	- n_iter: 4608 (no changes)
	
	Time:
	40 m
	
	Test performance:
	- threshold			: 0.904
	- distance			: 0.952
	- exact_accuracy 	: 0.540
	- balanced_accuracy	: 0.407
	
- 8-DT
	Candidates:
	- n_iter: 866250 --> 4950

	Time:
	17 s
	
	Test performance:
	- threshold			: 0.844
	- distance			: 0.932
	- exact_accuracy 	: 0.464
	- balanced_accuracy	: 0.359
	

- 8-NB
	Time:
	0.071 s
	
	Test performance:
	- threshold			: 0.918
	- distance			: 0.944
	- exact_accuracy 	: 0.493
	- balanced_accuracy	: 0.319

	NOTE:
	Uses CROSS_VALIDATION instead of RANDOM_SEARCH

- 8-NN
	Candidates:
	- n_iter: 360 --> 67

	Improvements:
	- Fixed L2 regularization
	- Adaptive learning rate
	- Early stopping
	- Fixed NN-tuple generation
	
	Time:
	54 m
	
	Test performance:
	- threshold			: 0.828
	- distance			: 0.804
	- exact_accuracy 	: 0.126
	- balanced_accuracy	: 0.000
	
	
	Config:
	- n_jobs: 10
	- activation: Added "logistic"
	- layers: (2,8,1)
	- layer_size: [10,15,20,25]
	- max_iter: 10000
	- alpha: [0.0001, 0.001, 0.01, 0.1]
	- tol: [0.0001]
	- learning_rate_init: [0.1]
	
- 8-RF
	Candidates:
	- n_iter: 9216 --> 4608
	
	Time:
	41 m
	
	Test performance:
	- threshold			: 0.904
	- distance			: 0.953
	- exact_accuracy 	: 0.550
	- balanced_accuracy	: 0.451
	
	
== NEW NEW EXPERIMENT ===
# Try to get the most juice out of the data using hyperparameter tuning methods: 
# - Grad Student Descent 
# - Grid Search

# GSD uses domain knowledge, experience from previous experiments, and arbitrary choices in an attempt to increase model accuracy 
# Purpose: How does using pipeline modules as hyperparameters affect model performance?

- 9-DT
	Candidates: 29700
	
	Time:
	2 m
	
	Test performance:
	- threshold			: 0.930
	- distance			: 0.957
	- exact_accuracy 	: 0.553
	- balanced_accuracy	: 0.463

	Config:
	- ImputationMethod: MODE

- 9-NN
	Time:
	24 s

	Test performance:
	- threshold			: 0.930
	- distance			: 0.957
	- exact_accuracy 	: 0.540
	- balanced_accuracy	: 0.498

	Config:
	- UseContinuousFeatures: true
	- Transformer:
	    DiscretizeMethod: CHIMERGE
	    UseOneHotEncoding: true
	    ImputationMethod: MODE
	- Model:
	    hidden_layer_sizes: [20,10,10]
		activation: tanh
		solver: adam
		learning_rate: adaptive
		learning_rate_init: 0.1
		batch_size: 2000
		alpha: 0.001
		max_iter: 10000
		tol: 0.0001
		n_iter_no_change: 20
	

- 9-RF
	Time:
	0.14 s
	
	Test performance:
	- threshold			: 0.921
	- distance			: 0.955
	- exact_accuracy 	: 0.570
	- balanced_accuracy	: 0.484

	Config:
	- OutlierRemovalMethod: ODIN
	- UseOneHotEncoding: true
	- ImputationMethod: MODE	
	- Model:
		"criterion": "log_loss",
		"max_depth": null,
		"min_samples_split": 2,
		"min_samples_leaf": 1,
		"min_weight_fraction_leaf": 0,
		"max_features": "sqrt",
		"max_leaf_nodes": null,
		"min_impurity_decrease": 0.0,
		"ccp_alpha": 0.0
		"n_estimators": 100,
		"bootstrap": true,
		"oob_score": false,
		"max_samples": null
	
	