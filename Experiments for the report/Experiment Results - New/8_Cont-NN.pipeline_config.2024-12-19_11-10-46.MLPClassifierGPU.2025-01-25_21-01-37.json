{
    "RandomSearchCV_BestParams": {
        "tol": 0.0001,
        "optimizer": "sgd",
        "n_iter_no_change": 20,
        "learning_rate_init": 0.1,
        "learning_rate": "adaptive",
        "hidden_layer_sizes": [
            10,
            10,
            10,
            10,
            10,
            10,
            10,
            10,
            10
        ],
        "epochs": 10000,
        "batch_size": 2000,
        "alpha": 0.001,
        "activation": "sigmoid"
    },
    "feature_importances": {
        "threshold": {
            "importances_mean": [
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0
            ],
            "importances_std": [
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0
            ],
            "importances": [
                [
                    0.0
                ],
                [
                    0.0
                ],
                [
                    0.0
                ],
                [
                    0.0
                ],
                [
                    0.0
                ],
                [
                    0.0
                ],
                [
                    0.0
                ],
                [
                    0.0
                ],
                [
                    0.0
                ],
                [
                    0.0
                ],
                [
                    0.0
                ],
                [
                    0.0
                ]
            ]
        },
        "distance": {
            "importances_mean": [
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0
            ],
            "importances_std": [
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0
            ],
            "importances": [
                [
                    0.0
                ],
                [
                    0.0
                ],
                [
                    0.0
                ],
                [
                    0.0
                ],
                [
                    0.0
                ],
                [
                    0.0
                ],
                [
                    0.0
                ],
                [
                    0.0
                ],
                [
                    0.0
                ],
                [
                    0.0
                ],
                [
                    0.0
                ],
                [
                    0.0
                ]
            ]
        },
        "exact_accuracy": {
            "importances_mean": [
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0
            ],
            "importances_std": [
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0
            ],
            "importances": [
                [
                    0.0
                ],
                [
                    0.0
                ],
                [
                    0.0
                ],
                [
                    0.0
                ],
                [
                    0.0
                ],
                [
                    0.0
                ],
                [
                    0.0
                ],
                [
                    0.0
                ],
                [
                    0.0
                ],
                [
                    0.0
                ],
                [
                    0.0
                ],
                [
                    0.0
                ]
            ]
        },
        "balanced_accuracy": {
            "importances_mean": [
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0
            ],
            "importances_std": [
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.0
            ],
            "importances": [
                [
                    0.0
                ],
                [
                    0.0
                ],
                [
                    0.0
                ],
                [
                    0.0
                ],
                [
                    0.0
                ],
                [
                    0.0
                ],
                [
                    0.0
                ],
                [
                    0.0
                ],
                [
                    0.0
                ],
                [
                    0.0
                ],
                [
                    0.0
                ],
                [
                    0.0
                ]
            ]
        }
    },
    "feature_names_in": [
        "Niveau s\u00e5rv\u00e6v",
        "S\u00e5rskorpe",
        "Granulationsv\u00e6v",
        "Epithelialisering",
        "Kontraktion",
        "Hyper\u00e6mi",
        "\u00d8dem",
        "Eksudat",
        "Eksudattype",
        "Infektionsniveau",
        "S\u00e5rrand (cm)",
        "Midte (cm)"
    ],
    "train_accuracies": {
        "threshold": 0.8208955223880597,
        "distance": 0.798673300165833,
        "exact_accuracy": 0.12437810945273632,
        "balanced_accuracy": 0.0
    },
    "train_precision": 0.12437810945273632,
    "train_recall": 0.12437810945273632,
    "test_accuracies": {
        "threshold": 0.8278145695364238,
        "distance": 0.803784295175023,
        "exact_accuracy": 0.12582781456953643,
        "balanced_accuracy": 0.0
    },
    "test_precision": 0.12582781456953643,
    "test_recall": 0.12582781456953643,
    "train_pred_y": [
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2
    ],
    "test_pred_y": [
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2,
        2
    ],
    "pipeline_config": {
        "General": {
            "loglevel": "INFO",
            "n_jobs": 10,
            "write_figure_to_disk": true,
            "UseCleaner": true,
            "UseStatisticalFeatureSelector": false,
            "UseTransformer": true,
            "UseOutlierRemoval": false,
            "UseContinuousFeatures": true
        },
        "DataPreprocessing": {
            "Cleaning": {
                "DeleteNanColumns": true,
                "DeleteNonfeatures": true,
                "DeleteMissingValues": false,
                "DeleteUndeterminedValue": false,
                "RemoveNaNAmount": true,
                "RemoveNaNAmountArgs": 3,
                "FillNan": true,
                "ShowNan": true
            },
            "OutlierAnalysis": {
                "OutlierRemovalMethod": "ODIN",
                "odinParams": {
                    "k": 30,
                    "T": 0
                },
                "avfParams": {
                    "k": 10
                }
            },
            "Transformer": {
                "Discretization": {
                    "DiscretizeColumns": [
                        "S\u00e5rrand (cm)",
                        "Midte (cm)"
                    ],
                    "DiscretizeMethod": "CHIMERGE",
                    "ChiMergeMaximumMergeThreshold": {
                        "S\u00e5rrand (cm)": "inf",
                        "Midte (cm)": "inf"
                    },
                    "DiscretizeDesiredIntervals": {
                        "S\u00e5rrand (cm)": 5,
                        "Midte (cm)": 5
                    }
                },
                "OneHotEncoding": {
                    "UseOneHotEncoding": false,
                    "OneHotEncodeLabels": [
                        "Eksudattype"
                    ]
                },
                "Imputation": {
                    "ImputationMethod": "KNN",
                    "KNN_NearestNeighbors": 5,
                    "KNN_DistanceMetric": "MATRIX"
                },
                "Normalisation": {
                    "NormalisationMethod": "NONE",
                    "NormaliseFeatures": [
                        "Niveau s\u00e5rv\u00e6v",
                        "S\u00e5rskorpe"
                    ]
                }
            },
            "StatisticalFeatureSelection": {
                "score_function": "MUTUAL_INFO_CLASSIFER",
                "MutualInfoClassifArgs": {
                    "discrete_features": true,
                    "n_neighbors": 3,
                    "random_state": 12
                },
                "GenericUnivariateSelect": true,
                "GenericUnivariateSelectArgs": {
                    "mode": "PERCENTILE",
                    "param": 50
                }
            }
        },
        "ModelSelection": {
            "model": "NEURAL_NETWORK",
            "DecisionTree": {
                "criterion": "gini",
                "max_depth": null,
                "min_samples_split": 2,
                "min_samples_leaf": 1,
                "min_weight_fraction_leaf": 0,
                "max_features": "sqrt",
                "random_state": 42,
                "max_leaf_nodes": null,
                "min_impurity_decrease": 0.0,
                "ccp_alpha": 0.0
            },
            "RandomForest": {
                "n_estimators": 100,
                "bootstrap": true,
                "oob_score": false,
                "random_state": 53,
                "max_samples": null
            },
            "CategoricalNaiveBayes": {
                "min_categories": 101
            },
            "NeuralNetwork": {
                "hidden_layer_sizes": [
                    10,
                    10
                ],
                "activation": "tanh",
                "solver": "adam",
                "learning_rate": "constant",
                "learning_rate_init": 0.001,
                "batch_size": 2000,
                "alpha": 0.001,
                "max_iter": 2000,
                "tol": 0.0001,
                "n_iter_no_change": 20,
                "random_state": 678
            }
        },
        "CrossValidationSelection": {
            "cross_validator": "STRATIFIED_KFOLD",
            "StratifiedKFold": {
                "n_splits": 5,
                "shuffle": true,
                "random_state": 177
            }
        },
        "ModelTraining": {
            "training_method": "RANDOM_SEARCH",
            "score_functions": [
                "ALL"
            ],
            "score_function_params": {
                "threshold": 20
            },
            "score_function_weights": {
                "threshold": 0.8,
                "distance": 0.9,
                "exact_accuracy": 1,
                "balanced_accuracy": 1.1
            },
            "train_test_split": {
                "random_state": 111,
                "train_size": 0.8
            },
            "PermutationFeatureImportance": {
                "n_repeats": 1,
                "random_state": 298
            },
            "RFECV": {
                "min_features_to_select": 1,
                "step": 1
            },
            "RandomizedSearchCV": {
                "n_iter": 67,
                "random_state": 378
            },
            "GridSearchCV": {
                "return_train_score": false,
                "refit": "exact_accuracy",
                "verbose": 1
            }
        },
        "ModelEvaluation": {
            "print_model_report": true,
            "plot_confusion_matrix": false,
            "plot_roc_curves": false,
            "plot_feature_importance": false,
            "plot_tree": false,
            "plot_decision_boundary": false
        }
    },
    "grid_config": {
        "ParamGrid": {
            "ParamGridDecisionTree": {
                "criterion": [
                    "log_loss"
                ],
                "max_depth": {
                    "start": 8,
                    "stop": 30,
                    "step": 4
                },
                "min_samples_split": {
                    "start": 7,
                    "stop": 10,
                    "step": 1
                },
                "min_samples_leaf": {
                    "start": 3,
                    "stop": 10,
                    "step": 2
                },
                "min_weight_fraction_leaf": {
                    "start": 0.0,
                    "stop": 0.1,
                    "step": 0.2
                },
                "max_features": [
                    "sqrt"
                ],
                "max_leaf_nodes": {
                    "start": 50,
                    "stop": 70,
                    "step": 4
                },
                "min_impurity_decrease": {
                    "start": 0.0,
                    "stop": 0.2,
                    "step": 0.3
                },
                "ccp_alpha": {
                    "start": 0.0,
                    "stop": 0.5,
                    "step": 0.9
                }
            },
            "ParamGridRandomForest": {
                "n_estimators": {
                    "start": 100,
                    "stop": 1000,
                    "step": 100
                },
                "bootstrap": [
                    true
                ],
                "oob_score": [
                    false
                ],
                "max_samples": {
                    "start": 200,
                    "stop": 1000,
                    "step": 100
                }
            },
            "ParamGridCategoricalNaiveBayes": {
                "min_categories": [
                    101
                ]
            },
            "ParamGridNeuralNetwork": {
                "hidden_layer_sizes": {
                    "layers": {
                        "start": 2,
                        "stop": 8,
                        "step": 1
                    },
                    "layer_size": [
                        10,
                        15,
                        20,
                        25
                    ]
                },
                "activation": [
                    "relu",
                    "tanh",
                    "logistic"
                ],
                "solver": [
                    "sgd",
                    "adam"
                ],
                "learning_rate": [
                    "adaptive"
                ],
                "learning_rate_init": [
                    0.1
                ],
                "batch_size": [
                    2000
                ],
                "alpha": [
                    0.0001,
                    0.001,
                    0.01,
                    0.1
                ],
                "max_iter": [
                    10000
                ],
                "tol": [
                    0.0001
                ],
                "n_iter_no_change": [
                    20
                ]
            }
        },
        "RandomParamGrid": {
            "RandomParamGridDecisionTree": {
                "criterion": [
                    "gini",
                    "log_loss"
                ],
                "max_depth": {
                    "dist": "RANDINT",
                    "dist_params": {
                        "low": 8,
                        "high": 30,
                        "size": 60
                    }
                },
                "min_samples_split": {
                    "dist": "RANDINT",
                    "dist_params": {
                        "low": 7,
                        "high": 10,
                        "size": 50
                    }
                },
                "min_samples_leaf": {
                    "dist": "RANDINT",
                    "dist_params": {
                        "low": 3,
                        "high": 10,
                        "size": 100
                    }
                },
                "min_weight_fraction_leaf": {
                    "dist": "RANDFLOAT",
                    "dist_params": {
                        "low": 0.0,
                        "high": 0.0,
                        "size": 100
                    }
                },
                "max_features": [
                    "sqrt",
                    "log2"
                ],
                "max_leaf_nodes": {
                    "dist": "RANDINT",
                    "dist_params": {
                        "low": 50,
                        "high": 70,
                        "size": 100
                    }
                },
                "min_impurity_decrease": {
                    "dist": "RANDFLOAT",
                    "dist_params": {
                        "low": 0.0,
                        "high": 0.0,
                        "size": 100
                    }
                },
                "ccp_alpha": {
                    "dist": "RANDFLOAT",
                    "dist_params": {
                        "low": 0.0,
                        "high": 0.0,
                        "size": 100
                    }
                }
            },
            "RandomParamGridRandomForest": {
                "n_estimators": {
                    "dist": "RANDINT",
                    "dist_params": {
                        "low": 100,
                        "high": 1000,
                        "size": 200
                    }
                },
                "bootstrap": [
                    true
                ],
                "oob_score": [
                    false
                ],
                "max_samples": {
                    "dist": "RANDINT",
                    "dist_params": {
                        "low": 200,
                        "high": 928,
                        "size": 200
                    }
                }
            },
            "RandomParamGridCategoricalNaiveBayes": {
                "min_categories": [
                    101
                ]
            },
            "RandomParamGridNeuralNetwork": {
                "hidden_layer_sizes": {
                    "layers": {
                        "start": 1,
                        "stop": 10,
                        "step": 1
                    },
                    "layer_size": [
                        10,
                        15,
                        20,
                        25
                    ]
                },
                "activation": [
                    "logistic",
                    "relu",
                    "tanh"
                ],
                "solver": [
                    "sgd",
                    "adam"
                ],
                "learning_rate": [
                    "adaptive"
                ],
                "learning_rate_init": [
                    0.1
                ],
                "batch_size": [
                    2000
                ],
                "alpha": [
                    0.0001,
                    0.001,
                    0.01,
                    0.1
                ],
                "max_iter": [
                    10000
                ],
                "tol": [
                    0.0001
                ],
                "n_iter_no_change": [
                    20
                ]
            }
        }
    }
}